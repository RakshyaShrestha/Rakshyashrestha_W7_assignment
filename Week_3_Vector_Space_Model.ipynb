{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pJc1LK44-XoW",
        "outputId": "65ffa4cc-fd2a-470c-c1b3-509c860f224e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Precision at K\n",
        "def precision_at_k(relevant_docs, retrieved_docs, k):\n",
        "    retrieved_at_k = retrieved_docs[:k]\n",
        "    relevant_and_retrieved = len([doc for doc in retrieved_at_k if doc in relevant_docs])\n",
        "    return relevant_and_retrieved / k\n",
        "\n",
        "# Recall at K\n",
        "def recall_at_k(relevant_docs, retrieved_docs, k):\n",
        "    retrieved_at_k = retrieved_docs[:k]\n",
        "    relevant_and_retrieved = len([doc for doc in retrieved_at_k if doc in relevant_docs])\n",
        "    return relevant_and_retrieved / len(relevant_docs) if relevant_docs else 0\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Directory containing the text documents\n",
        "    directory = '/content/drive/MyDrive/documents'\n",
        "\n",
        "    # Verify directory exists\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Error: The directory '{directory}' was not found.\")\n",
        "        return\n",
        "\n",
        "    # Reading all files from the directory\n",
        "    docs = []\n",
        "    filenames = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            try:\n",
        "                with open(filepath, \"r\") as file:\n",
        "                    content = file.read()\n",
        "                    docs.append(content)\n",
        "                    filenames.append(filename)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file '{filename}': {e}\")\n",
        "                return\n",
        "\n",
        "    if not docs:\n",
        "        print(\"No documents found in the directory.\")\n",
        "        return\n",
        "\n",
        "    # Hardcoded queries\n",
        "    queries = ['horror', 'sci-fi', 'drama']\n",
        "\n",
        "    # Define relevant documents per query (update with correct filenames from your dataset)\n",
        "    relevant_docs_per_query = {\n",
        "        'horror': ['horror_1.txt', 'horror_2.txt'],\n",
        "        'sci-fi': ['sci-fi_1.txt', 'sci-fi_2.txt'],\n",
        "        'drama': ['drama_1.txt', 'drama_2.txt']\n",
        "    }\n",
        "\n",
        "    # Use TfidfVectorizer to create TF-IDF vectors for both documents and queries\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit the vectorizer on the document collection (this creates a vocabulary and assigns TF-IDF values)\n",
        "    doc_tfidf_vectors = vectorizer.fit_transform(docs).toarray()\n",
        "\n",
        "    # Transform the queries into the same TF-IDF space\n",
        "    query_tfidf_vectors = vectorizer.transform(queries).toarray()\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    cosine_similarities = []\n",
        "    for query_vector in query_tfidf_vectors:\n",
        "        similarities = cosine_similarity(query_vector.reshape(1, -1), doc_tfidf_vectors).flatten()\n",
        "        cosine_similarities.append(similarities)\n",
        "\n",
        "    # Precision and Recall calculations at k=5\n",
        "    k = 5\n",
        "    for i, query in enumerate(queries):\n",
        "        print(f\"\\nResults for query '{query}':\")\n",
        "\n",
        "        # Pair document filenames with their similarity scores\n",
        "        doc_similarity_pairs = list(zip(filenames, cosine_similarities[i]))\n",
        "        # Sort by similarity in descending order\n",
        "        ranked_docs = sorted(doc_similarity_pairs, key=lambda x: x[1], reverse=True)\n",
        "        ranked_filenames = [doc[0] for doc in ranked_docs]\n",
        "\n",
        "        # Debugging output: Print top 5 ranked documents for each query\n",
        "        print(f\"Top {k} ranked documents for query '{query}':\")\n",
        "        for rank, (filename, score) in enumerate(ranked_docs[:k], 1):\n",
        "            print(f\"Rank {rank}: Document {filename} - Score: {score:.4f}\")\n",
        "\n",
        "        # Calculate precision and recall at k\n",
        "        precision_k = precision_at_k(relevant_docs_per_query.get(query, []), ranked_filenames, k)\n",
        "        recall_k = recall_at_k(relevant_docs_per_query.get(query, []), ranked_filenames, k)\n",
        "\n",
        "        print(f\"Precision at {k}: {precision_k:.4f}\")\n",
        "        print(f\"Recall at {k}: {recall_k:.4f}\")\n",
        "\n",
        "    # Naive Bayes Classification\n",
        "    labels = [1 if 'horror' in filename else 2 if 'sci-fi' in filename else 3 for filename in filenames]\n",
        "\n",
        "    # Fit the Naive Bayes model\n",
        "    nb_model = MultinomialNB()\n",
        "    X_train = np.array(doc_tfidf_vectors)\n",
        "    y_train = np.array(labels)\n",
        "    nb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict using Naive Bayes\n",
        "    X_test = np.array(query_tfidf_vectors)\n",
        "    y_pred = nb_model.predict(X_test)\n",
        "\n",
        "    # Expected labels for the queries\n",
        "    y_true = [1 if 'horror' in query else 2 if 'sci-fi' in query else 3 for query in queries]\n",
        "\n",
        "    # Calculate accuracy, precision, and recall\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\nNaive Bayes Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Naive Bayes Precision: {precision:.4f}\")\n",
        "    print(f\"Naive Bayes Recall: {recall:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "EdCawHehEDNC",
        "outputId": "dc4decd6-3cc5-4135-adb8-ab67dffa31a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for query 'horror':\n",
            "Top 5 ranked documents for query 'horror':\n",
            "Rank 1: Document The Walking Dead.txt - Score: 0.0784\n",
            "Rank 2: Document Speak No Evil.txt - Score: 0.0772\n",
            "Rank 3: Document Beetlejuice.txt - Score: 0.0705\n",
            "Rank 4: Document Poltergeist.txt - Score: 0.0698\n",
            "Rank 5: Document Transformers One.txt - Score: 0.0000\n",
            "Precision at 5: 0.0000\n",
            "Recall at 5: 0.0000\n",
            "\n",
            "Results for query 'sci-fi':\n",
            "Top 5 ranked documents for query 'sci-fi':\n",
            "Rank 1: Document Transformers One.txt - Score: 0.1394\n",
            "Rank 2: Document Poltergeist.txt - Score: 0.0000\n",
            "Rank 3: Document The Intouchables.txt - Score: 0.0000\n",
            "Rank 4: Document The Walking Dead.txt - Score: 0.0000\n",
            "Rank 5: Document WALLÂ·E.txt - Score: 0.0000\n",
            "Precision at 5: 0.0000\n",
            "Recall at 5: 0.0000\n",
            "\n",
            "Results for query 'drama':\n",
            "Top 5 ranked documents for query 'drama':\n",
            "Rank 1: Document Your Name (Kimi no Na w.txt - Score: 0.0627\n",
            "Rank 2: Document The Godfather.txt - Score: 0.0624\n",
            "Rank 3: Document The Intouchables.txt - Score: 0.0619\n",
            "Rank 4: Document Ikiru.txt - Score: 0.0616\n",
            "Rank 5: Document City of God.txt - Score: 0.0589\n",
            "Precision at 5: 0.0000\n",
            "Recall at 5: 0.0000\n",
            "\n",
            "Naive Bayes Accuracy: 0.3333\n",
            "Naive Bayes Precision: 0.1111\n",
            "Naive Bayes Recall: 0.3333\n"
          ]
        }
      ]
    }
  ]
}